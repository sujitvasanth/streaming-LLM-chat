<table>
<tr>
<td>

![samplechat](https://github.com/sujitvasanth/streaming-LLM-chat/blob/main/samplechat.gif)

</td>
<td>

### Streaming-LLM-chat

This is a transformers library application

that allows you to choose a local LLM and run

streaming inference on GPU.

it uses:

- Python: 3.8.10
- transformers library: 4.36.2
- transformers_stream_generator library

the models are predownloaded to the

oogabooga textgeneration ui folder
</body>
</td>
</tr>
</table>

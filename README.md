<table>
<tr>
<td>

![samplechat](https://github.com/sujitvasanth/streaming-LLM-chat/blob/main/samplechat.gif)

</td>
<td>

### Streaming-LLM-chat

<body>This is a transformers library application that

allows you to choose a local LLM and run

streaming inference on GPU.

it uses:



Python 3.8.10

Transformers version: 4.36.2

transformers_stream_generator


the models are predownloaded to the

oogabooga textgeneration ui folder
</body>
</td>
</tr>
</table>
